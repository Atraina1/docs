---
title: "Choosing your architecture"
---

Use these criteria to help you to select the appropriate Always On architecture.

|                             | Bronze | Silver | Gold | Platinum |
|-----------------------------|------------------|------------------|----------------|----------------------|
| Hardware failure protection | Yes              | Yes              | Yes            | Yes                  |
| Location failure protection | No (unless Barman is moved offsite)| Yes - Recovery from backup | Yes - instant failover to fully functional site | Yes - instant failover to fully functional site |
| Failover to DR or full DC   | DR (if Barman is located offsite); NA otherwise | DR (if Barman is located offsite) | Full DC | Full DC |
| Zero downtime upgrade       | Yes              | Yes              | Yes            | Yes                  |
| Support of availability zones in public/ private cloud | Yes  | Yes              | Yes            | Yes                  |
| Fast local restoration of high availability after device failure | No; time to restore HA: (1) VM prov + (2) approx 60 min/500GB | Yes; three local data nodes allow to maintain HA after device failure | No; time to restore HA: (1) VM prov + (2) approx 60 min/500GB | Yes; logical standbys can quickly be promoted to master data nodes |
| Cross data center network traffic | Backup traffic only (if Barman is located offsite); none otherwise | Backup traffic only (if Barman is located offsite); none otherwise | Full replication traffic | Full replication traffic |
| License cost            | 2 data nodes      | 3 data nodes      | 4 data nodes     | 4 data nodes <br/>2 logical standbys |

## Deployment and sizing considerations

For production deployments, EDB recommends a minimum of 12 cores for each Postgres data node and each logical standby. Witness nodes don't participate in the data replication operation and don't have to meet this requirement. Always size logical standbys exactly like the data nodes to avoid performance degradations in case of a node promotion. In production deployments, HARP proxy nodes require a minimum of four cores each. EDB recommends detailed benchmarking of performance requirements, working with EDBâ€™s Professional Services team.

For development purposes, don't assign Postgres data nodes fewer than two cores. The sizing of Barman nodes depends on the database size and the data change rate.

You can deploy Postgres data nodes, logical standbys, Barman nodes, and HARP proxy nodes on virtual machines or in a bare metal deployment mode. However, maintain anti-affinity between data nodes and HARP proxy nodes.  Don't deploy multiple data nodes on VMs that are on the same physical hardware, as that reduces resiliency.  Also don't deploy multiple HARP proxy nodes on VMs on the same physical hardware, as that, too, reduces resiliency.

You can deploy single HARP proxy nodes with single data nodes on the same physical hardware but should be deployed as VMs to ensure proper CPU and memory resource assignment.

## Clocks and timezones

EDB Postgres Distributed has been designed to operate with nodes in multiple timezones, allowing a truly worldwide database cluster. Individual servers do not need to be configured with matching timezones, though we do recommend using log_timezone = UTC to ensure the human readable server log is more accessible and comparable.

Server clocks should be synchronized using NTP or other solutions.

Clock synchronization is not critical to performance, as is the case with some other solutions. Clock skew can impact Origin Conflict Detection, though
EDB Postgres Distributed provides controls to report and manage any skew that exists. EDB Postgres Distributed also provides Row Version Conflict Detection, as described in [Conflict Detection](/bdr/conflicts).
