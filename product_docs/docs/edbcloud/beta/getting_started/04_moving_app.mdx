---
title: "A demonstration of EDB Advanced Server Oracle Compatibility on EDBCloud"
navTitle: "Oracle compatability"
description: "Migrating data into a new cluster and reconfiguring an Oracle-compatible application to use it"
tags:
  - epas
  - live-demo
katacodaPanel:
  account: notKC
  scenario: dbaasdemo
  codelanguages: shell, sql
showInteractiveBadge: true
---

After [creating a cluster](03_create_cluster), you can move data into it and [run applications against it](../using_cluster/02_connect_to_cluster/). If you choose to create an EDB Postgres Advanced Server cluster, this includes compatibility with much of the Oracle syntax and many built-in packages. 

This section demonstrates that compatibility, using a sample reporting application (an analytical query originally written against Oracle) and [a sample database](https://github.com/lerocha/chinook-database/blob/master/ChinookDatabase/DataSources/Chinook_Oracle.sql) (converted from Oracle via [the EDB Migration Toolkit](/migration_toolkit/latest/)).  

!!!interactive This demo is interactive
    You can follow along right in your browser by clicking the button below. Once the environment initializes, you'll see a terminal open at the bottom of the screen.

<KatacodaPanel />

## Demo: Oracle-compatible syntax

A critical question for people migrating applications from one platform to another is "How much will need to change?" While tools such as EDB's [Migration Portal](/migration_portal/latest/) and Migration Toolkit ease the burden of transforming the schema, data, and associated server-side logic, this still leaves open the question of whether or not application-side SQL queries will translate cleanly. 

Let's consider an application that searches for album titles in the [Chinook dataset](https://github.com/lerocha/chinook-database) and returns the list of tracks along with their size. We can implement this using a shell script:

```bash
#!/usr/bin/env bash

query="'%`echo $1 | tr '[:upper:]' '[:lower:]'`%'"


# https://stackoverflow.com/questions/3477213/is-there-for-the-bash-something-like-perls-data/3477269
sed '0,/^__SQL__$/d' $0 \
    | psql -w $PGDATABASE \
           -v search=$query 

exit

__SQL__
SELECT UNIQUE title, 
       ROUND(AVG(bytes) OVER (PARTITION BY mediatypeid)/1048576 ) media_avg_mb,
       LISTAGG(t.name || ' (' || ROUND(bytes/1048576) || ' mb)', chr(10)) 
         WITHIN GROUP (ORDER BY trackid)
         OVER (PARTITION BY title)  track_list 
FROM track t
JOIN album 
  USING (albumid)
JOIN mediatype
  USING (mediatypeid)
WHERE lower(title) LIKE :search
ORDER BY title;
```

This query uses two seperate [window functions](https://en.wikipedia.org/wiki/Window_function_(SQL)) (Oracle calls them [analytic functions](https://docs.oracle.com/en/database/oracle/oracle-database/21/sqlrf/Analytic-Functions.html)) that aggregate based on a group of rows. The first is `AVG(...) OVER (...)`, which has been a standard since [SQL:2003](https://en.wikipedia.org/wiki/SQL:2003). The query also uses `LISTAGG (...)`, which was introduced in Oracle 11g Release 2. Very few database systems support `LISTAGG`, but it is possible to [use a workaround such as `STRING_AGG`](https://www.enterprisedb.com/blog/how-workaround-oracle-listagg-function-postgresql). Unfortunately, `STRING_AGG` is an aggregate function requires rewriting the entire query to include a `GROUP BY` clause. Fortunately [EDB Postgres Advanced Server](https://www.enterprisedb.com/docs/epas/latest/) supports [`LISTAGG`](https://www.enterprisedb.com/docs/epas/latest/epas_compat_reference/02_the_sql_language/03_functions_and_operators/11_aggregate_functions/#listagg) so this query doesn't need to change when migrating from Oracle.

Here's what it looks like in action:

```shell
~/chinook_report.sh baby
__OUTPUT__
    title     | media_avg_mb |                    track_list                     
--------------+--------------+---------------------------------------------------
 Achtung Baby |            9 | Zoo Station (9 mb)                               +
              |              | Even Better Than The Real Thing (7 mb)           +
              |              | One (9 mb)                                       +
              |              | Until The End Of The World (9 mb)                +
              |              | Who's Gonna Ride Your Wild Horses (10 mb)        +
              |              | So Cruel (11 mb)                                 +
              |              | The Fly (8 mb)                                   +
              |              | Mysterious Ways (8 mb)                           +
              |              | Tryin' To Throw Your Arms Around The World (7 mb)+
              |              | Ultraviolet (Light My Way) (10 mb)               +
              |              | Acrobat (8 mb)                                   +
              |              | Love Is Blindness (8 mb)
(1 row)
```

This little "app" is configured to run against a Postgres Advanced Server cluster owned by EDB; the connection string is contained in the `PGDATABASE` environmental variable. Feel free to play around with it if you like:

```shell
psql $PGDATABASE
```

When you're done, quit psql with `\q`:

```shell
\q
```

Now let's move this data into your own cluster and reconfigure the script to run against it...

## Moving a database's schema and data into an EDB Cloud cluster

### Prerequesites

You'll need a fresh cluster configured for this demo (as described in [the previous section](03_create_cluster)), and the connection URL and admin password handy. We recommend creating a fresh cluster and password before you begin. For the purpose of this demo, please create a small cluster running EDB Postgres Advanced Server with public networking.

!!! Warning
    These instructions result in a publicly-accessible database and involve the use of the administrative account via
    a connection over the public Internet; you should be very cautious about any use of these features when handling sensitive
    or proprietary data. Prefer private links for connections whenever feasible!

Let's start by collecting the information needed to connect to your cluster. You'll need:

- **The host:** this will look like `something.edbmp.edbcloud.io` - you can find it on the Connect tab for your cluster on the portal. 
- **The port:** this will usually be `5432`, but you can double-check that on the Connect tab for your cluster as well.
- **The admin username:** this will be `edb_admin`, and again will be listed on the Connect tab.
- **The admin database name:** this will *also* be `edb_admin`, and also will be listed on the Connect tab.
- **Your admin password:** this will not be listed *anywhere* - you specified it while creating your cluster, and only you know what it is; you'll need it to connect.  

For convenience, let's put these - EXCEPT for the password - into variables:

```shell
read -p "Enter host: " DEST_HOST && \
read -e -i 5432 -p "Enter port: " DEST_PORT && \
read -e -i edb_admin -p "Enter username: " DEST_USER && \
read -e -i edb_admin -p "Enter database: " DEST_DB
```

### Connecting to the destination databases 

For the destination, you'll create a connection string using the information you collected above. For convenience, let's save those in another environmental variable: use the format `postgres://USERNAME@HOST:PORT/DATABASE?sslmode=require`. 

```shell
export PGDEST=postgres://$DEST_USER@$DEST_HOST:$DEST_PORT/$DEST_DB?sslmode=require
echo $PGDEST
__OUTPUT__
postgres://edb_admin@something.edbmp.edbcloud.io:5432/edb_admin?sslmode=require
```
Before moving on, please verify that you are able to connect to your cluster by using PSQL to list available databases (you'll be prompted to enter your password):

```shell
psql $PGDEST
__OUTPUT__
Password for user edb_admin: 
psql.bin (13.4.8 (Debian 13.4.8-1+deb10), server 13.4.8 (Debian 13.4.8-1+deb10))
SSL connection (protocol: TLSv1.3, cipher: TLS_AES_256_GCM_SHA384, bits: 256, compression: off)
Type "help" for help.

edb_admin=>
```

!!! Warning
    Even though you're working with this information in the privacy of your own account, please be mindful of the fact that you've now entering sensitive credentials into a shared sandbox environment; for safety, you should change your password 
    after you're done experimenting.
    EDB Cloud does not store your password and will not ask you for it; never share the admin password for
    a database.

Leave this connection open - we'll be using it to set up a new database & role next.

### Identifying the necessary roles and database

Before embarking on a migration, we'll need to audit the role(s) used by our application to ensure that they remain available. We should keep the principle of least-privilege in mind here, and avoid creating (or migrating) roles that are not strictly needed by our application.

As shown in the listings above, our reporting query needs to read data from tables in the "chinook" database. Let's start by creating that & connecting to it:

```sql
CREATE DATABASE chinook;
__OUTPUT__
CREATE DATABASE
```

```sql
\c chinook
__OUTPUT__
psql.bin (13.4.8 (Debian 13.4.8-1+deb10), server 13.4.8 (Debian 13.4.8-1+deb10))
SSL connection (protocol: TLSv1.3, cipher: TLS_AES_256_GCM_SHA384, bits: 256, compression: off)
You are now connected to database "chinook" as user "edb_admin".
chinook=>
```

The script needs to be able to select from tables in this database, but does not need to write to it, modify the schema, etc. So we'll need a read-only user: `chinook`. For convenience in this demo, we'll give it a password of `chinook`.

```sql
CREATE USER chinook WITH PASSWORD 'chinook';
__OUTPUT__
CREATE USER
```

Since this database will ONLY be used for this purpose, we'll grant this user select privileges on any future tables created in it:

```sql
ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON TABLES TO chinook;
```

The database is now ready for data, so exit psql.

```sql
\q
```

### Exporting schema and data

Next we'll use `pg_dump` to export the application's schema and data:

```shell
pg_dump --format custom $PGDATABASE > chinook.dump
```

This uses a binary format (`--format custom`) to efficiently capture the schema and data into the file `chinook.dump`. 
It will capture all objects associated with the database specified in the `$PGDATABASE` connection string (chinook). Note
that this may *potentially* include things that cannot be restored... You can view what's in the dump via 
	 
### Importing schema and data

Before we import the schema and data that we exported in the previous step, we'll want to modify the connection string to
point to the new `chinook` database. We *won't* use our newly-created user just yet, as we still need write access!

```shell
export PGDEST=postgres://$DEST_USER@$DEST_HOST:$DEST_PORT/chinook?sslmode=require
echo $PGDEST
__OUTPUT__
postgres://edb_admin@something.edbmp.edbcloud.io:5432/chinook?sslmode=require
```

Now we can bring in the schema and data using `pg_restore`; once again, you'll be prompted to enter your admin password for this:

```shell
pg_restore --no-owner -d $PGDEST chinook.dump
__OUTPUT__
Password: 
pg_restore: while PROCESSING TOC:
pg_restore: from TOC entry 4766; 0 0 COMMENT EXTENSION pg_stat_statements 
pg_restore: error: could not execute query: ERROR:  must be owner of extension pg_stat_statements
Command was: COMMENT ON EXTENSION pg_stat_statements IS 'track planning and execution statistics of all SQL statements executed';


pg_restore: warning: errors ignored on restore: 1
```

!!! Note The error is fine
    You can safely ignore this error: pg_stat_statements is a very useful extension
    and is installed by default on EDB Cloud clusters, but since we're running as
    the admin user and *not* a superuser, we can't modify it.
    The rest of the schema and data has been restored just fine however.

The option `--no-owner` ensures that pg_restore does not try to assign ownership of objects to their original owner
(which probably does not exist in our new database); instead, these objects will be owned by edb_admin, and thus safe 
from modification unless we connect as that user.

## Reconfiguring and testing application queries

Now all that remains is to point our application to the new database. Once again, we want a new connection string for this, 
one that points at the newly-created database and authenticates with the newly-created (read-access-only) user. This time, we'll overwrite the original connection string (stored in `PGDATABASE`):

```shell
export PGDATABASE="postgres://chinook:chinook@$DEST_HOST:$DEST_PORT/chinook?sslmode=require"
echo $PGDATABASE
__OUTPUT__
postgres://chinook:chinook@something.edbmp.edbcloud.io:5432/chinook?sslmode=require
```

Try it out:

```shell
~/chinook_report.sh baby
__OUTPUT__
    title     | media_avg_mb |                    track_list
--------------+--------------+---------------------------------------------------
 Achtung Baby |            9 | Zoo Station (9 mb)                               +
              |              | Even Better Than The Real Thing (7 mb)           +
              |              | One (9 mb)                                       +
              |              | Until The End Of The World (9 mb)                +
              |              | Who's Gonna Ride Your Wild Horses (10 mb)        +
              |              | So Cruel (11 mb)                                 +
              |              | The Fly (8 mb)                                   +
              |              | Mysterious Ways (8 mb)                           +
              |              | Tryin' To Throw Your Arms Around The World (7 mb)+
              |              | Ultraviolet (Light My Way) (10 mb)               +
              |              | Acrobat (8 mb)                                   +
              |              | Love Is Blindness (8 mb)
(1 row)
```

The same results as at the start, but now running against your own cluster!

## Next steps


 
- [Managing Postgres Access](../using_cluster/01_postgres_access/)
- [Backing Up and Restoring](../using_cluster/04_backup_and_restore/)
- ...